{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests pandas numpy matplotlib seaborn scipy requests-cache retry-requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c54566ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== IMPORTS ========\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7072f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DataCo daily sales...\n",
      "\n",
      "=== Sample DataCo Coordinates ===\n",
      "   latitude  longitude\n",
      "0    18.204    -66.371\n",
      "1    18.208    -66.371\n",
      "2    18.212    -66.371\n",
      "3    18.217    -66.371\n",
      "4    18.218    -66.371\n",
      "\n",
      "=== Processing Year 2015 ===\n",
      "Weather sample coords for 2015: [[17.982, -66.113], [18.007, -66.636], [18.018, -66.616], [18.025, -66.613], [18.025, -66.615]]\n",
      "Saved merged dataset for 2015: ./merged_data_final\\DataCo_Weather_2015_Merged_new.csv (Rows: 122814)\n",
      "\n",
      "=== Processing Year 2016 ===\n",
      "Weather sample coords for 2016: [[17.982, -66.113], [18.007, -66.636], [18.025, -66.613], [18.025, -66.615], [18.033, -66.852]]\n",
      "Saved merged dataset for 2016: ./merged_data_final\\DataCo_Weather_2016_Merged_new.csv (Rows: 89084)\n",
      "\n",
      "=== Processing Year 2017 ===\n",
      "Weather sample coords for 2017: [[-33.938, 18.571], [17.982, -66.113], [18.007, -66.636], [18.018, -66.616], [18.025, -66.613]]\n",
      "Saved merged dataset for 2017: ./merged_data_final\\DataCo_Weather_2017_Merged_new.csv (Rows: 323515)\n"
     ]
    }
   ],
   "source": [
    "# Merges cleaned Data_Co_Daily_By_Location.csv (lat/lon in degrees) with Visual Crossing weather files (2015–2017)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Config ===\n",
    "SALES_FILE = \"Data_Co_Daily_By_Location.csv\"      # Daily aggregated sales with degrees lat/lon\n",
    "WEATHER_DIR = \"./cleaned_weather\"     # Cleaned weather files for 2015–2017\n",
    "OUTPUT_DIR = \"./merged_data_final\"    # Output folder\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "YEARS = [2015, 2016, 2017]\n",
    "\n",
    "# Column names\n",
    "SALES_DATE_COL = \"date_only\"\n",
    "WEATHER_DATE_COL = \"datetime\"\n",
    "LAT_SALES = \"Latitude\"\n",
    "LON_SALES = \"Longitude\"\n",
    "\n",
    "# === Load DataCo Sales ===\n",
    "print(\"Loading DataCo daily sales...\")\n",
    "sales_df = pd.read_csv(SALES_FILE)\n",
    "\n",
    "# Parse and normalize date\n",
    "sales_df[SALES_DATE_COL] = pd.to_datetime(sales_df[SALES_DATE_COL], errors='coerce').dt.normalize()\n",
    "\n",
    "# Rename lat/lon to match weather files\n",
    "sales_df = sales_df.rename(columns={LAT_SALES: \"latitude\", LON_SALES: \"longitude\"})\n",
    "\n",
    "# Round lat/lon for matching\n",
    "sales_df[\"latitude\"] = sales_df[\"latitude\"].round(3)\n",
    "sales_df[\"longitude\"] = sales_df[\"longitude\"].round(3)\n",
    "\n",
    "print(\"\\n=== Sample DataCo Coordinates ===\")\n",
    "print(sales_df[['latitude','longitude']].drop_duplicates().head())\n",
    "\n",
    "# === Process each year ===\n",
    "for year in YEARS:\n",
    "    weather_file = os.path.join(WEATHER_DIR, f\"visualcrossing_weather_{year}_cleaned.csv\")\n",
    "    if not os.path.exists(weather_file):\n",
    "        print(f\"\\nSkipping {year} - weather file not found\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== Processing Year {year} ===\")\n",
    "    weather_df = pd.read_csv(weather_file)\n",
    "\n",
    "    # Parse and normalize weather date\n",
    "    weather_df[WEATHER_DATE_COL] = pd.to_datetime(weather_df[WEATHER_DATE_COL], errors='coerce').dt.normalize()\n",
    "\n",
    "    # Round weather lat/lon for matching\n",
    "    weather_df[\"latitude\"] = weather_df[\"latitude\"].round(3)\n",
    "    weather_df[\"longitude\"] = weather_df[\"longitude\"].round(3)\n",
    "\n",
    "    print(f\"Weather sample coords for {year}:\", weather_df[['latitude','longitude']].drop_duplicates().head().values.tolist())\n",
    "\n",
    "    # Filter sales data for current year\n",
    "    sales_year = sales_df[sales_df[SALES_DATE_COL].dt.year == year].copy()\n",
    "\n",
    "    # Merge on date + lat/lon\n",
    "    merged = pd.merge(\n",
    "        sales_year,\n",
    "        weather_df,\n",
    "        left_on=[SALES_DATE_COL, \"latitude\", \"longitude\"],\n",
    "        right_on=[WEATHER_DATE_COL, \"latitude\", \"longitude\"],\n",
    "        how=\"left\"\n",
    "    ).drop(columns=[WEATHER_DATE_COL])\n",
    "\n",
    "    # Save merged file\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"DataCo_Weather_{year}_Merged_new.csv\")\n",
    "    merged.to_csv(output_file, index=False)\n",
    "    print(f\"Saved merged dataset for {year}: {output_file} (Rows: {merged.shape[0]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e25182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all year-wise merged files...\n",
      "Saved combined dataset: ./final_datasets/DataCo_Weather_2015_2017_All_new.csv (Rows: 535413)\n"
     ]
    }
   ],
   "source": [
    "# Concatenates DataCo + Weather merged files (2015–2017) into one dataset\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Config ===\n",
    "MERGED_DIR = \"./merged_data_final\"    # Folder containing year-wise merged files\n",
    "OUTPUT_FILE = \"./final_datasets/DataCo_Weather_2015_2017_All_new.csv\"\n",
    "os.makedirs(\"./final_datasets\", exist_ok=True)\n",
    "\n",
    "# File paths\n",
    "file_2015 = os.path.join(MERGED_DIR, \"DataCo_Weather_2015_Merged_new.csv\")\n",
    "file_2016 = os.path.join(MERGED_DIR, \"DataCo_Weather_2016_Merged_new.csv\")\n",
    "file_2017 = os.path.join(MERGED_DIR, \"DataCo_Weather_2017_Merged_new.csv\")\n",
    "\n",
    "# Load and concatenate all years\n",
    "print(\"Loading all year-wise merged files...\")\n",
    "df_2015 = pd.read_csv(file_2015)\n",
    "df_2016 = pd.read_csv(file_2016)\n",
    "df_2017 = pd.read_csv(file_2017)\n",
    "\n",
    "all_df = pd.concat([df_2015, df_2016, df_2017], ignore_index=True)\n",
    "\n",
    "# Save the combined dataset\n",
    "all_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Saved combined dataset: {OUTPUT_FILE} (Rows: {all_df.shape[0]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78dbaa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading combined dataset...\n",
      "Saved dataset with Sales + Temp lag features: ./final_datasets/DataCo_Weather_Lagged_new.csv (Rows: 535413)\n"
     ]
    }
   ],
   "source": [
    "# Adds lag features (1, 7, 30 days) for Sales and Temperature to DataCo + Weather dataset.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Config ===\n",
    "INPUT_FILE = \"./final_datasets/DataCo_Weather_2015_2017_All_new.csv\"\n",
    "OUTPUT_FILE = \"./final_datasets/DataCo_Weather_Lagged_new.csv\"\n",
    "\n",
    "# Columns to lag\n",
    "TARGET_COLS = [\"Sales\", \"temp\"]  \n",
    "\n",
    "# Lag periods (in days)\n",
    "LAGS = [1, 7, 30]\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading combined dataset...\")\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# Ensure date is datetime\n",
    "df[\"date_only\"] = pd.to_datetime(df[\"date_only\"], errors='coerce')\n",
    "\n",
    "# Sort by location and date\n",
    "df = df.sort_values(by=[\"latitude\", \"longitude\", \"date_only\"])\n",
    "\n",
    "# Generate lag features for each target variable\n",
    "for col in TARGET_COLS:\n",
    "    for lag in LAGS:\n",
    "        df[f\"{col}_lag_{lag}\"] = df.groupby([\"latitude\", \"longitude\"])[col].shift(lag)\n",
    "\n",
    "# Save output\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Saved dataset with Sales + Temp lag features: {OUTPUT_FILE} (Rows: {df.shape[0]})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

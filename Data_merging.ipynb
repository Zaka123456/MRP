{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests pandas numpy matplotlib seaborn scipy requests-cache retry-requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c54566ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== IMPORTS ========\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "004b6870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing visualcrossing_weather_2015.csv ===\n",
      "                Missing Values  Missing %\n",
      "preciptype              657199      50.75\n",
      "solarradiation           48910       3.78\n",
      "tempmin                      0       0.00\n",
      "temp                         0       0.00\n",
      "datetime                     0       0.00\n",
      "tempmax                      0       0.00\n",
      "precip                       0       0.00\n",
      "humidity                     0       0.00\n",
      "windspeed                    0       0.00\n",
      "snow                         0       0.00\n",
      "cloudcover                   0       0.00\n",
      "latitude                     0       0.00\n",
      "longitude                    0       0.00\n",
      "year                         0       0.00\n",
      "Missing value report saved as Weather_Missing_Report_2015.csv\n",
      "\n",
      "=== Processing visualcrossing_weather_2016.csv ===\n",
      "                Missing Values  Missing %\n",
      "preciptype              495070      49.06\n",
      "solarradiation           58194       5.77\n",
      "tempmin                      0       0.00\n",
      "temp                         0       0.00\n",
      "datetime                     0       0.00\n",
      "tempmax                      0       0.00\n",
      "precip                       0       0.00\n",
      "humidity                     0       0.00\n",
      "windspeed                    0       0.00\n",
      "snow                         0       0.00\n",
      "cloudcover                   0       0.00\n",
      "latitude                     0       0.00\n",
      "longitude                    0       0.00\n",
      "year                         0       0.00\n",
      "Missing value report saved as Weather_Missing_Report_2016.csv\n",
      "\n",
      "=== Processing visualcrossing_weather_2017.csv ===\n",
      "                Missing Values  Missing %\n",
      "preciptype             2024967      48.70\n",
      "solarradiation          142560       3.43\n",
      "tempmin                      0       0.00\n",
      "temp                         0       0.00\n",
      "datetime                     0       0.00\n",
      "tempmax                      0       0.00\n",
      "precip                       0       0.00\n",
      "humidity                     0       0.00\n",
      "windspeed                    0       0.00\n",
      "snow                         0       0.00\n",
      "cloudcover                   0       0.00\n",
      "latitude                     0       0.00\n",
      "longitude                    0       0.00\n",
      "year                         0       0.00\n",
      "Missing value report saved as Weather_Missing_Report_2017.csv\n"
     ]
    }
   ],
   "source": [
    "# VS Code script to analyze missing values for 2015, 2016, and 2017 Visual Crossing weather datasets\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Config ===\n",
    "YEARS = [2015, 2016, 2017]\n",
    "CHUNKSIZE = 200000  # Safe size for large CSVs\n",
    "INPUT_DIR = \".\"\n",
    "\n",
    "def check_missing_values(file_path):\n",
    "    total_counts = None\n",
    "    missing_counts = None\n",
    "\n",
    "    for chunk in pd.read_csv(file_path, chunksize=CHUNKSIZE, low_memory=False):\n",
    "        if total_counts is None:\n",
    "            total_counts = chunk.count()\n",
    "            missing_counts = chunk.isnull().sum()\n",
    "        else:\n",
    "            total_counts += chunk.count()\n",
    "            missing_counts += chunk.isnull().sum()\n",
    "\n",
    "    # Estimate total rows (average across columns)\n",
    "    total_rows = sum(total_counts) / len(total_counts)\n",
    "    \n",
    "    # Build report\n",
    "    missing_report = pd.DataFrame({\n",
    "        \"Missing Values\": missing_counts,\n",
    "        \"Missing %\": (missing_counts / total_rows * 100).round(2)\n",
    "    }).sort_values(by=\"Missing %\", ascending=False)\n",
    "\n",
    "    return missing_report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for year in YEARS:\n",
    "        file_name = f\"visualcrossing_weather_{year}.csv\"\n",
    "        if not os.path.exists(file_name):\n",
    "            print(f\"Skipping {year} - file not found: {file_name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n=== Processing {file_name} ===\")\n",
    "        report = check_missing_values(file_name)\n",
    "        print(report)\n",
    "        \n",
    "        # Save a report for each year\n",
    "        output_name = f\"Weather_Missing_Report_{year}.csv\"\n",
    "        report.to_csv(output_name)\n",
    "        print(f\"Missing value report saved as {output_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13e6413a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2015...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zaka and Family\\AppData\\Local\\Temp\\ipykernel_21848\\3203080287.py:34: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  group[SOLAR_COL] = group[SOLAR_COL].fillna(method='ffill').fillna(method='bfill')\n",
      "C:\\Users\\Zaka and Family\\AppData\\Local\\Temp\\ipykernel_21848\\3203080287.py:65: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby([LAT_COL, LON_COL], group_keys=False).apply(fill_solar)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned weather file for 2015: ./cleaned_weather\\visualcrossing_weather_2015_cleaned.csv\n",
      "Processing 2016...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zaka and Family\\AppData\\Local\\Temp\\ipykernel_21848\\3203080287.py:34: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  group[SOLAR_COL] = group[SOLAR_COL].fillna(method='ffill').fillna(method='bfill')\n",
      "C:\\Users\\Zaka and Family\\AppData\\Local\\Temp\\ipykernel_21848\\3203080287.py:65: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby([LAT_COL, LON_COL], group_keys=False).apply(fill_solar)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned weather file for 2016: ./cleaned_weather\\visualcrossing_weather_2016_cleaned.csv\n",
      "Processing 2017...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zaka and Family\\AppData\\Local\\Temp\\ipykernel_21848\\3203080287.py:34: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  group[SOLAR_COL] = group[SOLAR_COL].fillna(method='ffill').fillna(method='bfill')\n",
      "C:\\Users\\Zaka and Family\\AppData\\Local\\Temp\\ipykernel_21848\\3203080287.py:65: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby([LAT_COL, LON_COL], group_keys=False).apply(fill_solar)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned weather file for 2017: ./cleaned_weather\\visualcrossing_weather_2017_cleaned.csv\n",
      "\n",
      "All weather files (2015–2017) have been cleaned and saved in ./cleaned_weather\n"
     ]
    }
   ],
   "source": [
    "# Cleans Visual Crossing weather data (2015–2017):\n",
    "# - Fills missing solarradiation with hybrid method (interpolation → rolling mean → ffill/bfill)\n",
    "# - Fills preciptype using temperature + precipitation rules\n",
    "# - Adds binary flags for rain, snow, freezingrain\n",
    "# - Outputs cleaned weather files for each year\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Configuration ===\n",
    "YEARS = [2015, 2016, 2017]\n",
    "INPUT_DIR = \".\"   \n",
    "OUTPUT_DIR = \"./cleaned_weather\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Column names\n",
    "DATE_COL = \"datetime\"\n",
    "LAT_COL = \"latitude\"\n",
    "LON_COL = \"longitude\"\n",
    "SOLAR_COL = \"solarradiation\"\n",
    "PRECIP_COL = \"precip\"\n",
    "TEMP_COL = \"temp\"\n",
    "PRECIPTYPE_COL = \"preciptype\"\n",
    "\n",
    "# === Step 1: Hybrid Fill for Solar Radiation ===\n",
    "def fill_solar(group):\n",
    "    # Linear interpolation first (handles most gaps)\n",
    "    group[SOLAR_COL] = group[SOLAR_COL].interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    # Rolling mean (7-day window) for any remaining NaNs\n",
    "    group[SOLAR_COL] = group[SOLAR_COL].fillna(group[SOLAR_COL].rolling(window=7, min_periods=1).mean())\n",
    "    \n",
    "    # Forward/backward fill as final fallback (edges only)\n",
    "    group[SOLAR_COL] = group[SOLAR_COL].fillna(method='ffill').fillna(method='bfill')\n",
    "    return group\n",
    "\n",
    "# === Step 2: Fill Precipitation Type (Categorical) ===\n",
    "def infer_preciptype(row):\n",
    "    # Keep existing label if present\n",
    "    if pd.notnull(row[PRECIPTYPE_COL]):\n",
    "        return row[PRECIPTYPE_COL]\n",
    "    \n",
    "    # If no precipitation\n",
    "    if pd.isnull(row[PRECIP_COL]) or row[PRECIP_COL] == 0:\n",
    "        return \"none\"\n",
    "    \n",
    "    # Infer based on temperature\n",
    "    if row[TEMP_COL] < 0:\n",
    "        return \"snow\"\n",
    "    elif 0 <= row[TEMP_COL] <= 2:\n",
    "        return \"freezingrain\"\n",
    "    else:\n",
    "        return \"rain\"\n",
    "\n",
    "# === Main Cleaning Function ===\n",
    "def clean_weather_file(year):\n",
    "    input_file = os.path.join(INPUT_DIR, f\"visualcrossing_weather_{year}.csv\")\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"visualcrossing_weather_{year}_cleaned.csv\")\n",
    "    \n",
    "    print(f\"Processing {year}...\")\n",
    "    df = pd.read_csv(input_file, parse_dates=[DATE_COL])\n",
    "    df = df.sort_values(by=[LAT_COL, LON_COL, DATE_COL])\n",
    "    \n",
    "    # Fill solar radiation with hybrid method\n",
    "    df = df.groupby([LAT_COL, LON_COL], group_keys=False).apply(fill_solar)\n",
    "    \n",
    "    # Fill preciptype using rules\n",
    "    df[PRECIPTYPE_COL] = df.apply(infer_preciptype, axis=1)\n",
    "    df[PRECIPTYPE_COL] = df[PRECIPTYPE_COL].fillna(\"unknown\")\n",
    "    \n",
    "    # Add binary weather flags for ML/RL features\n",
    "    df[\"is_rain\"] = (df[PRECIPTYPE_COL] == \"rain\").astype(int)\n",
    "    df[\"is_snow\"] = (df[PRECIPTYPE_COL] == \"snow\").astype(int)\n",
    "    df[\"is_freezingrain\"] = (df[PRECIPTYPE_COL] == \"freezingrain\").astype(int)\n",
    "    \n",
    "    # Save cleaned file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved cleaned weather file for {year}: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for year in YEARS:\n",
    "        clean_weather_file(year)\n",
    "    print(\"\\nAll weather files (2015–2017) have been cleaned and saved in ./cleaned_weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c0b1292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Aggregating by day...\n",
      "Saving aggregated data...\n",
      "Saved aggregated daily data to: Data_Co_Daily.csv\n"
     ]
    }
   ],
   "source": [
    "# group Data_Co.csv by day for merging with daily weather data \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# === Configuration ===\n",
    "INPUT_FILE = \"Data_Co_cleaned.csv\"          # Path to raw DataCo dataset\n",
    "OUTPUT_FILE = \"Data_Co_Daily.csv\"   # Output aggregated file\n",
    "DATE_COLUMN = \"order_date\"          \n",
    "\n",
    "# === Step 1: Load the data (handles large files in chunks if needed) ===\n",
    "def load_data(file_path, chunksize=100000):\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize, low_memory=False):\n",
    "        chunks.append(chunk)\n",
    "    return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# === Step 2: Process and group by day ===\n",
    "def group_by_day(df, date_col):\n",
    "    # Ensure date column is in datetime format\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "\n",
    "    # Drop rows where date couldn't be parsed\n",
    "    df = df.dropna(subset=[date_col])\n",
    "\n",
    "    # Extract just the date (no time)\n",
    "    df[\"date_only\"] = df[date_col].dt.date\n",
    "\n",
    "    # Group by day, summing numerical columns (e.g., sales, quantity)\n",
    "    daily_df = df.groupby(\"date_only\").sum(numeric_only=True).reset_index()\n",
    "\n",
    "    return daily_df\n",
    "\n",
    "# === Step 3: Save the result ===\n",
    "def save_data(df, output_file):\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved aggregated daily data to: {output_file}\")\n",
    "\n",
    "# === Main ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading data...\")\n",
    "    data = load_data(INPUT_FILE)\n",
    "\n",
    "    print(\"Aggregating by day...\")\n",
    "    daily_data = group_by_day(data, DATE_COLUMN)\n",
    "\n",
    "    print(\"Saving aggregated data...\")\n",
    "    save_data(daily_data, OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30ac1a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sales data...\n",
      "\n",
      "Merging 2015 sales with weather...\n",
      "Saved merged dataset for 2015: ./merged_data\\DataCo_Weather_2015_Merged.csv\n",
      "\n",
      "Merging 2016 sales with weather...\n",
      "Saved merged dataset for 2016: ./merged_data\\DataCo_Weather_2016_Merged.csv\n",
      "\n",
      "Merging 2017 sales with weather...\n",
      "Saved merged dataset for 2017: ./merged_data\\DataCo_Weather_2017_Merged.csv\n"
     ]
    }
   ],
   "source": [
    "# Merges Data_Co_Daily.csv with Visual Crossing weather files (2015–2017)\n",
    "# Standardizes date & coordinate columns before merging\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Config ===\n",
    "SALES_FILE = \"Data_Co_Daily.csv\"  # Daily aggregated sales (all years)\n",
    "WEATHER_DIR = \"./cleaned_weather\"  # Where cleaned weather files are stored\n",
    "OUTPUT_DIR = \"./merged_data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "YEARS = [2015, 2016, 2017]\n",
    "\n",
    "# Column names\n",
    "SALES_DATE_COL = \"date_only\"\n",
    "WEATHER_DATE_COL = \"datetime\"\n",
    "LAT_SALES = \"Latitude\"\n",
    "LON_SALES = \"Longitude\"\n",
    "LAT_WEATHER = \"latitude\"\n",
    "LON_WEATHER = \"longitude\"\n",
    "\n",
    "# Load sales data\n",
    "print(\"Loading sales data...\")\n",
    "sales_df = pd.read_csv(SALES_FILE)\n",
    "\n",
    "# Convert date to datetime object (normalize formats)\n",
    "sales_df[SALES_DATE_COL] = pd.to_datetime(sales_df[SALES_DATE_COL], format=\"%m/%d/%Y\", errors='coerce')\n",
    "\n",
    "# Standardize coordinate names in sales\n",
    "sales_df = sales_df.rename(columns={LAT_SALES: \"latitude\", LON_SALES: \"longitude\"})\n",
    "\n",
    "# Process year by year\n",
    "for year in YEARS:\n",
    "    weather_file = os.path.join(WEATHER_DIR, f\"visualcrossing_weather_{year}_cleaned.csv\")\n",
    "    if not os.path.exists(weather_file):\n",
    "        print(f\"Weather file missing for {year}: {weather_file}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nMerging {year} sales with weather...\")\n",
    "    weather_df = pd.read_csv(weather_file)\n",
    "    \n",
    "    # Convert weather date\n",
    "    weather_df[WEATHER_DATE_COL] = pd.to_datetime(weather_df[WEATHER_DATE_COL], format=\"%m/%d/%Y\", errors='coerce')\n",
    "\n",
    "    # Filter sales for this year only\n",
    "    sales_year = sales_df[sales_df[SALES_DATE_COL].dt.year == year].copy()\n",
    "\n",
    "    # Merge on date + coordinates\n",
    "    merged = pd.merge(\n",
    "        sales_year,\n",
    "        weather_df,\n",
    "        left_on=[SALES_DATE_COL, \"latitude\", \"longitude\"],\n",
    "        right_on=[WEATHER_DATE_COL, \"latitude\", \"longitude\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Drop duplicate date column\n",
    "    merged = merged.drop(columns=[WEATHER_DATE_COL])\n",
    "\n",
    "    # Save merged file\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"DataCo_Weather_{year}_Merged.csv\")\n",
    "    merged.to_csv(output_file, index=False)\n",
    "    print(f\"Saved merged dataset for {year}: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c38c0f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sales data...\n",
      "\n",
      "=== Sales Data Info ===\n",
      "Rows: 1127\n",
      "Unique dates: 0\n",
      "Unique lat/lon pairs: 1127\n",
      "Sample sales dates: <DatetimeArray>\n",
      "[]\n",
      "Length: 0, dtype: datetime64[ns]\n",
      "Sample sales coords:\n",
      "       latitude     longitude\n",
      "0  4896.563650 -13865.259369\n",
      "1  5034.414862 -13375.784661\n",
      "2  5440.104700 -14952.370637\n",
      "3  5239.624569 -15943.433639\n",
      "4  4344.302576 -13448.148567\n",
      "\n",
      "=== Debugging Year 2015 ===\n",
      "Weather Rows: 1345390\n",
      "Unique dates: 0\n",
      "Unique lat/lon pairs: 3686\n",
      "Sample weather dates: <DatetimeArray>\n",
      "[]\n",
      "Length: 0, dtype: datetime64[ns]\n",
      "Sample weather coords:\n",
      "        latitude  longitude\n",
      "0     17.982491 -66.112671\n",
      "365   18.006821 -66.635864\n",
      "730   18.018362 -66.616234\n",
      "1095  18.025232 -66.613274\n",
      "1460  18.025280 -66.613068\n",
      "Common dates between sales & weather (2015): 1\n",
      "Common lat/lon pairs between sales & weather (2015): 0\n",
      "Sample common date: [NaT]\n",
      "⚠️ No matches for 2015! Likely date format or coordinate mismatch.\n",
      "\n",
      "=== Debugging Year 2016 ===\n",
      "Weather Rows: 1048575\n",
      "Unique dates: 0\n",
      "Unique lat/lon pairs: 2865\n",
      "Sample weather dates: <DatetimeArray>\n",
      "[]\n",
      "Length: 0, dtype: datetime64[ns]\n",
      "Sample weather coords:\n",
      "        latitude  longitude\n",
      "0     17.982491 -66.112671\n",
      "366   18.006821 -66.635864\n",
      "732   18.025232 -66.613274\n",
      "1098  18.025280 -66.613068\n",
      "1464  18.025299 -66.613045\n",
      "Common dates between sales & weather (2016): 1\n",
      "Common lat/lon pairs between sales & weather (2016): 0\n",
      "Sample common date: [NaT]\n",
      "⚠️ No matches for 2016! Likely date format or coordinate mismatch.\n",
      "\n",
      "=== Debugging Year 2017 ===\n",
      "Weather Rows: 4313205\n",
      "Unique dates: 0\n",
      "Unique lat/lon pairs: 11817\n",
      "Sample weather dates: <DatetimeArray>\n",
      "[]\n",
      "Length: 0, dtype: datetime64[ns]\n",
      "Sample weather coords:\n",
      "        latitude  longitude\n",
      "0    -33.937553  18.571438\n",
      "365   17.982491 -66.112671\n",
      "730   18.006821 -66.635864\n",
      "1095  18.018362 -66.616234\n",
      "1460  18.025204 -66.612892\n",
      "Common dates between sales & weather (2017): 1\n",
      "Common lat/lon pairs between sales & weather (2017): 0\n",
      "Sample common date: [NaT]\n",
      "⚠️ No matches for 2017! Likely date format or coordinate mismatch.\n"
     ]
    }
   ],
   "source": [
    "# Debug script to identify why merged files are empty\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Config ===\n",
    "SALES_FILE = \"Data_Co_Daily.csv\"\n",
    "WEATHER_DIR = \"./cleaned_weather\"\n",
    "YEARS = [2015, 2016, 2017]\n",
    "\n",
    "# Column names\n",
    "SALES_DATE_COL = \"date_only\"\n",
    "WEATHER_DATE_COL = \"datetime\"\n",
    "LAT_SALES = \"Latitude\"\n",
    "LON_SALES = \"Longitude\"\n",
    "\n",
    "# Load sales data\n",
    "print(\"Loading sales data...\")\n",
    "sales_df = pd.read_csv(SALES_FILE)\n",
    "\n",
    "# Convert date to datetime\n",
    "sales_df[SALES_DATE_COL] = pd.to_datetime(sales_df[SALES_DATE_COL], format=\"%m/%d/%Y\", errors='coerce')\n",
    "\n",
    "# Rename lat/lon to match weather files\n",
    "sales_df = sales_df.rename(columns={LAT_SALES: \"latitude\", LON_SALES: \"longitude\"})\n",
    "\n",
    "# Check overall sales summary\n",
    "print(\"\\n=== Sales Data Info ===\")\n",
    "print(f\"Rows: {sales_df.shape[0]}\")\n",
    "print(f\"Unique dates: {sales_df[SALES_DATE_COL].nunique()}\")\n",
    "print(f\"Unique lat/lon pairs: {sales_df[['latitude','longitude']].drop_duplicates().shape[0]}\")\n",
    "print(\"Sample sales dates:\", sales_df[SALES_DATE_COL].dropna().unique()[:5])\n",
    "print(\"Sample sales coords:\\n\", sales_df[['latitude','longitude']].drop_duplicates().head())\n",
    "\n",
    "for year in YEARS:\n",
    "    weather_file = os.path.join(WEATHER_DIR, f\"visualcrossing_weather_{year}_cleaned.csv\")\n",
    "    if not os.path.exists(weather_file):\n",
    "        print(f\"\\nSkipping {year} - weather file not found\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== Debugging Year {year} ===\")\n",
    "    weather_df = pd.read_csv(weather_file)\n",
    "\n",
    "    # Convert weather date\n",
    "    weather_df[WEATHER_DATE_COL] = pd.to_datetime(weather_df[WEATHER_DATE_COL], format=\"%m/%d/%Y\", errors='coerce')\n",
    "\n",
    "    # Show weather summary\n",
    "    print(f\"Weather Rows: {weather_df.shape[0]}\")\n",
    "    print(f\"Unique dates: {weather_df[WEATHER_DATE_COL].nunique()}\")\n",
    "    print(f\"Unique lat/lon pairs: {weather_df[['latitude','longitude']].drop_duplicates().shape[0]}\")\n",
    "    print(\"Sample weather dates:\", weather_df[WEATHER_DATE_COL].dropna().unique()[:5])\n",
    "    print(\"Sample weather coords:\\n\", weather_df[['latitude','longitude']].drop_duplicates().head())\n",
    "\n",
    "    # Check intersection of dates\n",
    "    common_dates = set(sales_df[SALES_DATE_COL].dt.date).intersection(set(weather_df[WEATHER_DATE_COL].dt.date))\n",
    "    print(f\"Common dates between sales & weather ({year}): {len(common_dates)}\")\n",
    "\n",
    "    # Check intersection of coordinates\n",
    "    sales_coords = set([tuple(x) for x in sales_df[['latitude','longitude']].drop_duplicates().values])\n",
    "    weather_coords = set([tuple(x) for x in weather_df[['latitude','longitude']].drop_duplicates().values])\n",
    "    common_coords = sales_coords.intersection(weather_coords)\n",
    "    print(f\"Common lat/lon pairs between sales & weather ({year}): {len(common_coords)}\")\n",
    "\n",
    "    # If needed, list some sample common values\n",
    "    if common_dates:\n",
    "        print(\"Sample common date:\", list(common_dates)[:3])\n",
    "    if common_coords:\n",
    "        print(\"Sample common coords:\", list(common_coords)[:3])\n",
    "\n",
    "    if not common_dates or not common_coords:\n",
    "        print(f\"⚠️ No matches for {year}! Likely date format or coordinate mismatch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dd26a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sales data...\n",
      "\n",
      "=== Sales Data Summary ===\n",
      "Rows: 1127\n",
      "Unique dates: 0\n",
      "Unique lat/lon pairs: 1127\n",
      "Sample sales dates: []\n",
      "Sample sales coords:\n",
      "       latitude     longitude\n",
      "0  4896.563650 -13865.259369\n",
      "1  5034.414862 -13375.784661\n",
      "2  5440.104700 -14952.370637\n",
      "3  5239.624569 -15943.433639\n",
      "4  4344.302576 -13448.148567\n",
      "\n",
      "=== Processing Year 2015 ===\n",
      "Weather Rows: 1345390\n",
      "Unique dates: 0\n",
      "Unique lat/lon pairs: 3686\n",
      "Sample weather dates: []\n",
      "Sample weather coords:\n",
      "        latitude  longitude\n",
      "0     17.982491 -66.112671\n",
      "365   18.006821 -66.635864\n",
      "730   18.018362 -66.616234\n",
      "1095  18.025232 -66.613274\n",
      "1460  18.025280 -66.613068\n",
      "Raw common dates: 1\n",
      "Raw common coords: 0\n",
      "Common coords (after rounding): 0\n",
      "Saved merged file for 2015: ./merged_data\\DataCo_Weather_2015_Merged.csv\n",
      "Final merged rows for 2015: 0\n",
      "\n",
      "=== Processing Year 2016 ===\n",
      "Weather Rows: 1048575\n",
      "Unique dates: 0\n",
      "Unique lat/lon pairs: 2865\n",
      "Sample weather dates: []\n",
      "Sample weather coords:\n",
      "        latitude  longitude\n",
      "0     17.982491 -66.112671\n",
      "366   18.006821 -66.635864\n",
      "732   18.025232 -66.613274\n",
      "1098  18.025280 -66.613068\n",
      "1464  18.025299 -66.613045\n",
      "Raw common dates: 1\n",
      "Raw common coords: 0\n",
      "Common coords (after rounding): 0\n",
      "Saved merged file for 2016: ./merged_data\\DataCo_Weather_2016_Merged.csv\n",
      "Final merged rows for 2016: 0\n",
      "\n",
      "=== Processing Year 2017 ===\n",
      "Weather Rows: 4313205\n",
      "Unique dates: 0\n",
      "Unique lat/lon pairs: 11817\n",
      "Sample weather dates: []\n",
      "Sample weather coords:\n",
      "        latitude  longitude\n",
      "0    -33.937553  18.571438\n",
      "365   17.982491 -66.112671\n",
      "730   18.006821 -66.635864\n",
      "1095  18.018362 -66.616234\n",
      "1460  18.025204 -66.612892\n",
      "Raw common dates: 1\n",
      "Raw common coords: 0\n",
      "Common coords (after rounding): 0\n",
      "Saved merged file for 2017: ./merged_data\\DataCo_Weather_2017_Merged.csv\n",
      "Final merged rows for 2017: 0\n"
     ]
    }
   ],
   "source": [
    "# Debug + Fix script to merge Data_Co_Daily.csv with Visual Crossing weather data (2015–2017)\n",
    "# - Prints debug info (date & coordinate mismatches)\n",
    "# - Normalizes dates and rounds coordinates\n",
    "# - Outputs merged files for each year\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Config ===\n",
    "SALES_FILE = \"Data_Co_Daily.csv\"\n",
    "WEATHER_DIR = \"./cleaned_weather\"   # Cleaned weather files for 2015–2017\n",
    "OUTPUT_DIR = \"./merged_data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "YEARS = [2015, 2016, 2017]\n",
    "\n",
    "# Column names\n",
    "SALES_DATE_COL = \"date_only\"\n",
    "WEATHER_DATE_COL = \"datetime\"\n",
    "LAT_SALES = \"Latitude\"\n",
    "LON_SALES = \"Longitude\"\n",
    "\n",
    "# === Load and preprocess sales data ===\n",
    "print(\"Loading sales data...\")\n",
    "sales_df = pd.read_csv(SALES_FILE)\n",
    "\n",
    "# Convert date to datetime, normalize to YYYY-MM-DD\n",
    "sales_df[SALES_DATE_COL] = pd.to_datetime(sales_df[SALES_DATE_COL], format=\"%m/%d/%Y\", errors='coerce').dt.normalize()\n",
    "\n",
    "# Rename coordinates to match weather\n",
    "sales_df = sales_df.rename(columns={LAT_SALES: \"latitude\", LON_SALES: \"longitude\"})\n",
    "\n",
    "# Debug sales summary\n",
    "print(\"\\n=== Sales Data Summary ===\")\n",
    "print(f\"Rows: {sales_df.shape[0]}\")\n",
    "print(f\"Unique dates: {sales_df[SALES_DATE_COL].nunique()}\")\n",
    "print(f\"Unique lat/lon pairs: {sales_df[['latitude','longitude']].drop_duplicates().shape[0]}\")\n",
    "print(\"Sample sales dates:\", sales_df[SALES_DATE_COL].dropna().dt.strftime('%Y-%m-%d').unique()[:5])\n",
    "print(\"Sample sales coords:\\n\", sales_df[['latitude','longitude']].drop_duplicates().head())\n",
    "\n",
    "for year in YEARS:\n",
    "    weather_file = os.path.join(WEATHER_DIR, f\"visualcrossing_weather_{year}_cleaned.csv\")\n",
    "    if not os.path.exists(weather_file):\n",
    "        print(f\"\\nSkipping {year} - weather file not found\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== Processing Year {year} ===\")\n",
    "    weather_df = pd.read_csv(weather_file)\n",
    "\n",
    "    # Convert weather date, normalize\n",
    "    weather_df[WEATHER_DATE_COL] = pd.to_datetime(weather_df[WEATHER_DATE_COL], format=\"%m/%d/%Y\", errors='coerce').dt.normalize()\n",
    "\n",
    "    # Debug weather summary\n",
    "    print(f\"Weather Rows: {weather_df.shape[0]}\")\n",
    "    print(f\"Unique dates: {weather_df[WEATHER_DATE_COL].nunique()}\")\n",
    "    print(f\"Unique lat/lon pairs: {weather_df[['latitude','longitude']].drop_duplicates().shape[0]}\")\n",
    "    print(\"Sample weather dates:\", weather_df[WEATHER_DATE_COL].dropna().dt.strftime('%Y-%m-%d').unique()[:5])\n",
    "    print(\"Sample weather coords:\\n\", weather_df[['latitude','longitude']].drop_duplicates().head())\n",
    "\n",
    "    # Step 1: Check raw intersections\n",
    "    raw_dates = set(sales_df[SALES_DATE_COL].dt.date)\n",
    "    raw_weather_dates = set(weather_df[WEATHER_DATE_COL].dt.date)\n",
    "    common_dates_raw = raw_dates.intersection(raw_weather_dates)\n",
    "\n",
    "    sales_coords_raw = set([tuple(x) for x in sales_df[['latitude','longitude']].drop_duplicates().values])\n",
    "    weather_coords_raw = set([tuple(x) for x in weather_df[['latitude','longitude']].drop_duplicates().values])\n",
    "    common_coords_raw = sales_coords_raw.intersection(weather_coords_raw)\n",
    "\n",
    "    print(f\"Raw common dates: {len(common_dates_raw)}\")\n",
    "    print(f\"Raw common coords: {len(common_coords_raw)}\")\n",
    "\n",
    "    # Step 2: Fix coordinates by rounding to 3 decimals\n",
    "    sales_df[\"latitude\"] = sales_df[\"latitude\"].round(3)\n",
    "    sales_df[\"longitude\"] = sales_df[\"longitude\"].round(3)\n",
    "    weather_df[\"latitude\"] = weather_df[\"latitude\"].round(3)\n",
    "    weather_df[\"longitude\"] = weather_df[\"longitude\"].round(3)\n",
    "\n",
    "    sales_coords = set([tuple(x) for x in sales_df[['latitude','longitude']].drop_duplicates().values])\n",
    "    weather_coords = set([tuple(x) for x in weather_df[['latitude','longitude']].drop_duplicates().values])\n",
    "    common_coords = sales_coords.intersection(weather_coords)\n",
    "\n",
    "    print(f\"Common coords (after rounding): {len(common_coords)}\")\n",
    "    if common_coords:\n",
    "        print(\"Sample common coords:\", list(common_coords)[:5])\n",
    "\n",
    "    # Step 3: Merge (year-specific sales only)\n",
    "    sales_year = sales_df[sales_df[SALES_DATE_COL].dt.year == year].copy()\n",
    "\n",
    "    merged = pd.merge(\n",
    "        sales_year,\n",
    "        weather_df,\n",
    "        left_on=[SALES_DATE_COL, \"latitude\", \"longitude\"],\n",
    "        right_on=[WEATHER_DATE_COL, \"latitude\", \"longitude\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Drop duplicate date column from weather\n",
    "    merged = merged.drop(columns=[WEATHER_DATE_COL])\n",
    "\n",
    "    # Save merged dataset\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"DataCo_Weather_{year}_Merged.csv\")\n",
    "    merged.to_csv(output_file, index=False)\n",
    "    print(f\"Saved merged file for {year}: {output_file}\")\n",
    "    print(f\"Final merged rows for {year}: {merged.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "930a8a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sales data...\n",
      "\n",
      "=== Raw Data Samples (before parsing) ===\n",
      "Sample date_only values: ['2015-01-01', '2015-01-02', '2015-01-03', '2015-01-04', '2015-01-05']\n",
      "Sample Latitude/Longitude values: [[4896.56365017, -13865.25936908], [5034.41486184, -13375.78466069], [5440.1047001, -14952.37063659], [5239.6245690000005, -15943.43363927], [4344.30257636, -13448.14856739]]\n",
      "\n",
      "Detected projected coordinates (not degrees) — converting from Web Mercator (EPSG:3857) to lat/lon...\n",
      "\n",
      "=== Fixed Data Samples (after conversion) ===\n",
      "Parsed dates sample: <DatetimeArray>\n",
      "['2015-01-01 00:00:00', '2015-01-02 00:00:00', '2015-01-03 00:00:00',\n",
      " '2015-01-04 00:00:00', '2015-01-05 00:00:00']\n",
      "Length: 5, dtype: datetime64[ns]\n",
      "Latitude/Longitude sample:    latitude  longitude\n",
      "0  0.043987  -0.124554\n",
      "1  0.045225  -0.120157\n",
      "2  0.048869  -0.134319\n",
      "3  0.047068  -0.143222\n",
      "4  0.039026  -0.120807\n",
      "\n",
      "=== Year 2015 Debug ===\n",
      "Sales rows for 2015: 365\n",
      "Weather rows: 1345390\n",
      "Common dates: 365\n",
      "Common coords (after rounding): 0\n",
      "Saved merged dataset for 2015: ./merged_data_fixed\\DataCo_Weather_2015_Merged.csv (Rows: 365)\n",
      "\n",
      "=== Year 2016 Debug ===\n",
      "Sales rows for 2016: 366\n",
      "Weather rows: 1048575\n",
      "Common dates: 366\n",
      "Common coords (after rounding): 0\n",
      "Saved merged dataset for 2016: ./merged_data_fixed\\DataCo_Weather_2016_Merged.csv (Rows: 366)\n",
      "\n",
      "=== Year 2017 Debug ===\n",
      "Sales rows for 2017: 365\n",
      "Weather rows: 4313205\n",
      "Common dates: 365\n",
      "Common coords (after rounding): 0\n",
      "Saved merged dataset for 2017: ./merged_data_fixed\\DataCo_Weather_2017_Merged.csv (Rows: 365)\n"
     ]
    }
   ],
   "source": [
    "# Inspects and fixes Data_Co_Daily.csv (date format & coordinates) before merging with weather files (2015–2017)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "# === Config ===\n",
    "SALES_FILE = \"Data_Co_Daily.csv\"\n",
    "WEATHER_DIR = \"./cleaned_weather\"\n",
    "OUTPUT_DIR = \"./merged_data_fixed\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "YEARS = [2015, 2016, 2017]\n",
    "SALES_DATE_COL = \"date_only\"\n",
    "WEATHER_DATE_COL = \"datetime\"\n",
    "LAT_COL = \"Latitude\"\n",
    "LON_COL = \"Longitude\"\n",
    "\n",
    "# --- Helper: Convert projected (EPSG:3857 Web Mercator) coords to lat/lon ---\n",
    "def mercator_to_latlon(x, y):\n",
    "    lon = x / 20037508.34 * 180\n",
    "    lat = y / 20037508.34 * 180\n",
    "    lat = 180 / math.pi * (2 * math.atan(math.exp(lat * math.pi / 180)) - math.pi / 2)\n",
    "    return lat, lon\n",
    "\n",
    "# --- Load Sales Data ---\n",
    "print(\"Loading sales data...\")\n",
    "sales_df = pd.read_csv(SALES_FILE)\n",
    "\n",
    "# Inspect raw date and coordinates\n",
    "print(\"\\n=== Raw Data Samples (before parsing) ===\")\n",
    "print(\"Sample date_only values:\", sales_df[SALES_DATE_COL].head().tolist())\n",
    "print(\"Sample Latitude/Longitude values:\", sales_df[[LAT_COL, LON_COL]].head().values.tolist())\n",
    "\n",
    "# Try parsing date\n",
    "sales_df[SALES_DATE_COL] = pd.to_datetime(sales_df[SALES_DATE_COL], errors='coerce')\n",
    "\n",
    "# If all failed, try alternative format (day-first)\n",
    "if sales_df[SALES_DATE_COL].isna().all():\n",
    "    sales_df[SALES_DATE_COL] = pd.to_datetime(sales_df[SALES_DATE_COL], format=\"%d/%m/%Y\", errors='coerce')\n",
    "\n",
    "# Normalize dates (remove time)\n",
    "sales_df[SALES_DATE_COL] = sales_df[SALES_DATE_COL].dt.normalize()\n",
    "\n",
    "# Check if coordinates look like degrees (valid ranges: lat -90 to 90, lon -180 to 180)\n",
    "if (sales_df[LAT_COL].abs() > 90).any() or (sales_df[LON_COL].abs() > 180).any():\n",
    "    print(\"\\nDetected projected coordinates (not degrees) — converting from Web Mercator (EPSG:3857) to lat/lon...\")\n",
    "    latlon = sales_df.apply(lambda r: mercator_to_latlon(r[LON_COL], r[LAT_COL]), axis=1)\n",
    "    sales_df[\"latitude\"] = [p[0] for p in latlon]\n",
    "    sales_df[\"longitude\"] = [p[1] for p in latlon]\n",
    "else:\n",
    "    sales_df = sales_df.rename(columns={LAT_COL: \"latitude\", LON_COL: \"longitude\"})\n",
    "\n",
    "# Check fixed samples\n",
    "print(\"\\n=== Fixed Data Samples (after conversion) ===\")\n",
    "print(\"Parsed dates sample:\", sales_df[SALES_DATE_COL].dropna().unique()[:5])\n",
    "print(\"Latitude/Longitude sample:\", sales_df[['latitude','longitude']].head())\n",
    "\n",
    "# --- Merge Year by Year ---\n",
    "for year in YEARS:\n",
    "    weather_file = os.path.join(WEATHER_DIR, f\"visualcrossing_weather_{year}_cleaned.csv\")\n",
    "    if not os.path.exists(weather_file):\n",
    "        print(f\"\\nSkipping {year} - weather file not found\")\n",
    "        continue\n",
    "\n",
    "    weather_df = pd.read_csv(weather_file)\n",
    "    weather_df[WEATHER_DATE_COL] = pd.to_datetime(weather_df[WEATHER_DATE_COL], errors='coerce').dt.normalize()\n",
    "\n",
    "    # Round coordinates for matching\n",
    "    sales_df[\"latitude\"] = sales_df[\"latitude\"].round(3)\n",
    "    sales_df[\"longitude\"] = sales_df[\"longitude\"].round(3)\n",
    "    weather_df[\"latitude\"] = weather_df[\"latitude\"].round(3)\n",
    "    weather_df[\"longitude\"] = weather_df[\"longitude\"].round(3)\n",
    "\n",
    "    # Filter sales for current year\n",
    "    sales_year = sales_df[sales_df[SALES_DATE_COL].dt.year == year].copy()\n",
    "\n",
    "    # Debug intersections\n",
    "    common_dates = set(sales_year[SALES_DATE_COL].dt.date).intersection(set(weather_df[WEATHER_DATE_COL].dt.date))\n",
    "    sales_coords = set([tuple(x) for x in sales_year[['latitude','longitude']].drop_duplicates().values])\n",
    "    weather_coords = set([tuple(x) for x in weather_df[['latitude','longitude']].drop_duplicates().values])\n",
    "    common_coords = sales_coords.intersection(weather_coords)\n",
    "\n",
    "    print(f\"\\n=== Year {year} Debug ===\")\n",
    "    print(f\"Sales rows for {year}: {sales_year.shape[0]}\")\n",
    "    print(f\"Weather rows: {weather_df.shape[0]}\")\n",
    "    print(f\"Common dates: {len(common_dates)}\")\n",
    "    print(f\"Common coords (after rounding): {len(common_coords)}\")\n",
    "\n",
    "    # Merge\n",
    "    merged = pd.merge(\n",
    "        sales_year,\n",
    "        weather_df,\n",
    "        left_on=[SALES_DATE_COL, \"latitude\", \"longitude\"],\n",
    "        right_on=[WEATHER_DATE_COL, \"latitude\", \"longitude\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    merged = merged.drop(columns=[WEATHER_DATE_COL])\n",
    "\n",
    "    # Save output\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"DataCo_Weather_{year}_Merged.csv\")\n",
    "    merged.to_csv(output_file, index=False)\n",
    "    print(f\"Saved merged dataset for {year}: {output_file} (Rows: {merged.shape[0]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea209ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw DataCo data...\n",
      "Saved fixed daily aggregated file with proper lat/lon: Data_Co_Daily_new.csv\n"
     ]
    }
   ],
   "source": [
    "# Aggregates Data_Co.csv by day, preserving proper lat/lon in degrees.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_FILE = \"Data_Co_cleaned.csv\"\n",
    "OUTPUT_FILE = \"Data_Co_Daily_new.csv\"\n",
    "DATE_COLUMN = \"order_date\"\n",
    "\n",
    "print(\"Loading raw DataCo data...\")\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# Convert order date\n",
    "df[DATE_COLUMN] = pd.to_datetime(df[DATE_COLUMN], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid dates\n",
    "df = df.dropna(subset=[DATE_COLUMN])\n",
    "\n",
    "# Extract date only\n",
    "df[\"date_only\"] = df[DATE_COLUMN].dt.date\n",
    "\n",
    "# Group by date, latitude, longitude (keeping coordinates in degrees)\n",
    "agg_cols = {\n",
    "    col: 'sum' for col in df.columns if col not in [DATE_COLUMN, \"date_only\", \"Latitude\", \"Longitude\"]\n",
    "}\n",
    "# Keep first lat/lon per group (assuming consistent location per row)\n",
    "agg_cols[\"Latitude\"] = 'first'\n",
    "agg_cols[\"Longitude\"] = 'first'\n",
    "\n",
    "df_daily = df.groupby([\"date_only\"], as_index=False).agg(agg_cols)\n",
    "\n",
    "df_daily.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Saved fixed daily aggregated file with proper lat/lon: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7072f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DataCo daily sales...\n",
      "\n",
      "=== Sample DataCo Coordinates ===\n",
      "   latitude  longitude\n",
      "0    39.565    -87.289\n",
      "1    18.226    -66.047\n",
      "2    18.245    -66.371\n",
      "3    40.286    -75.089\n",
      "4    18.228    -66.044\n",
      "\n",
      "=== Processing Year 2015 ===\n",
      "Weather sample coords for 2015: [[17.982, -66.113], [18.007, -66.636], [18.018, -66.616], [18.025, -66.613], [18.025, -66.615]]\n",
      "Saved merged dataset for 2015: ./merged_data_final\\DataCo_Weather_2015_Merged.csv (Rows: 3090)\n",
      "\n",
      "=== Processing Year 2016 ===\n",
      "Weather sample coords for 2016: [[17.982, -66.113], [18.007, -66.636], [18.025, -66.613], [18.025, -66.615], [18.033, -66.852]]\n",
      "Saved merged dataset for 2016: ./merged_data_final\\DataCo_Weather_2016_Merged.csv (Rows: 1755)\n",
      "\n",
      "=== Processing Year 2017 ===\n",
      "Weather sample coords for 2017: [[-33.938, 18.571], [17.982, -66.113], [18.007, -66.636], [18.018, -66.616], [18.025, -66.613]]\n",
      "Saved merged dataset for 2017: ./merged_data_final\\DataCo_Weather_2017_Merged.csv (Rows: 7994)\n"
     ]
    }
   ],
   "source": [
    "# Merges cleaned Data_Co_Daily_new.csv (lat/lon in degrees) with Visual Crossing weather files (2015–2017)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Config ===\n",
    "SALES_FILE = \"Data_Co_Daily_new.csv\"      # Daily aggregated sales with degrees lat/lon\n",
    "WEATHER_DIR = \"./cleaned_weather\"     # Cleaned weather files for 2015–2017\n",
    "OUTPUT_DIR = \"./merged_data_final\"    # Output folder\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "YEARS = [2015, 2016, 2017]\n",
    "\n",
    "# Column names\n",
    "SALES_DATE_COL = \"date_only\"\n",
    "WEATHER_DATE_COL = \"datetime\"\n",
    "LAT_SALES = \"Latitude\"\n",
    "LON_SALES = \"Longitude\"\n",
    "\n",
    "# === Load DataCo Sales ===\n",
    "print(\"Loading DataCo daily sales...\")\n",
    "sales_df = pd.read_csv(SALES_FILE)\n",
    "\n",
    "# Parse and normalize date\n",
    "sales_df[SALES_DATE_COL] = pd.to_datetime(sales_df[SALES_DATE_COL], errors='coerce').dt.normalize()\n",
    "\n",
    "# Rename lat/lon to match weather files\n",
    "sales_df = sales_df.rename(columns={LAT_SALES: \"latitude\", LON_SALES: \"longitude\"})\n",
    "\n",
    "# Round lat/lon for matching\n",
    "sales_df[\"latitude\"] = sales_df[\"latitude\"].round(3)\n",
    "sales_df[\"longitude\"] = sales_df[\"longitude\"].round(3)\n",
    "\n",
    "print(\"\\n=== Sample DataCo Coordinates ===\")\n",
    "print(sales_df[['latitude','longitude']].drop_duplicates().head())\n",
    "\n",
    "# === Process each year ===\n",
    "for year in YEARS:\n",
    "    weather_file = os.path.join(WEATHER_DIR, f\"visualcrossing_weather_{year}_cleaned.csv\")\n",
    "    if not os.path.exists(weather_file):\n",
    "        print(f\"\\nSkipping {year} - weather file not found\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== Processing Year {year} ===\")\n",
    "    weather_df = pd.read_csv(weather_file)\n",
    "\n",
    "    # Parse and normalize weather date\n",
    "    weather_df[WEATHER_DATE_COL] = pd.to_datetime(weather_df[WEATHER_DATE_COL], errors='coerce').dt.normalize()\n",
    "\n",
    "    # Round weather lat/lon for matching\n",
    "    weather_df[\"latitude\"] = weather_df[\"latitude\"].round(3)\n",
    "    weather_df[\"longitude\"] = weather_df[\"longitude\"].round(3)\n",
    "\n",
    "    print(f\"Weather sample coords for {year}:\", weather_df[['latitude','longitude']].drop_duplicates().head().values.tolist())\n",
    "\n",
    "    # Filter sales data for current year\n",
    "    sales_year = sales_df[sales_df[SALES_DATE_COL].dt.year == year].copy()\n",
    "\n",
    "    # Merge on date + lat/lon\n",
    "    merged = pd.merge(\n",
    "        sales_year,\n",
    "        weather_df,\n",
    "        left_on=[SALES_DATE_COL, \"latitude\", \"longitude\"],\n",
    "        right_on=[WEATHER_DATE_COL, \"latitude\", \"longitude\"],\n",
    "        how=\"left\"\n",
    "    ).drop(columns=[WEATHER_DATE_COL])\n",
    "\n",
    "    # Save merged file\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"DataCo_Weather_{year}_Merged.csv\")\n",
    "    merged.to_csv(output_file, index=False)\n",
    "    print(f\"Saved merged dataset for {year}: {output_file} (Rows: {merged.shape[0]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e25182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all year-wise merged files...\n",
      "Saved combined dataset: ./final_datasets/DataCo_Weather_2015_2017_All.csv (Rows: 12839)\n"
     ]
    }
   ],
   "source": [
    "# Concatenates DataCo + Weather merged files (2015–2017) into one dataset\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Config ===\n",
    "MERGED_DIR = \"./merged_data_final\"    # Folder containing year-wise merged files\n",
    "OUTPUT_FILE = \"./final_datasets/DataCo_Weather_2015_2017_All.csv\"\n",
    "os.makedirs(\"./final_datasets\", exist_ok=True)\n",
    "\n",
    "# File paths\n",
    "file_2015 = os.path.join(MERGED_DIR, \"DataCo_Weather_2015_Merged.csv\")\n",
    "file_2016 = os.path.join(MERGED_DIR, \"DataCo_Weather_2016_Merged.csv\")\n",
    "file_2017 = os.path.join(MERGED_DIR, \"DataCo_Weather_2017_Merged.csv\")\n",
    "\n",
    "# Load and concatenate all years\n",
    "print(\"Loading all year-wise merged files...\")\n",
    "df_2015 = pd.read_csv(file_2015)\n",
    "df_2016 = pd.read_csv(file_2016)\n",
    "df_2017 = pd.read_csv(file_2017)\n",
    "\n",
    "all_df = pd.concat([df_2015, df_2016, df_2017], ignore_index=True)\n",
    "\n",
    "# Save the combined dataset\n",
    "all_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Saved combined dataset: {OUTPUT_FILE} (Rows: {all_df.shape[0]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78dbaa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading combined dataset...\n",
      "Saved dataset with Sales + Temp lag features: ./final_datasets/DataCo_Weather_Lagged.csv (Rows: 12839)\n"
     ]
    }
   ],
   "source": [
    "# Adds lag features (1, 7, 30 days) for Sales and Temperature to DataCo + Weather dataset.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Config ===\n",
    "INPUT_FILE = \"./final_datasets/DataCo_Weather_2015_2017_All.csv\"\n",
    "OUTPUT_FILE = \"./final_datasets/DataCo_Weather_Lagged.csv\"\n",
    "\n",
    "# Columns to lag\n",
    "TARGET_COLS = [\"Sales\", \"temp\"]  # Change \"Sales\" if your sales column has a different name\n",
    "\n",
    "# Lag periods (in days)\n",
    "LAGS = [1, 7, 30]\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading combined dataset...\")\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# Ensure date is datetime\n",
    "df[\"date_only\"] = pd.to_datetime(df[\"date_only\"], errors='coerce')\n",
    "\n",
    "# Sort by location and date\n",
    "df = df.sort_values(by=[\"latitude\", \"longitude\", \"date_only\"])\n",
    "\n",
    "# Generate lag features for each target variable\n",
    "for col in TARGET_COLS:\n",
    "    for lag in LAGS:\n",
    "        df[f\"{col}_lag_{lag}\"] = df.groupby([\"latitude\", \"longitude\"])[col].shift(lag)\n",
    "\n",
    "# Save output\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Saved dataset with Sales + Temp lag features: {OUTPUT_FILE} (Rows: {df.shape[0]})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
